{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assistive Vision Image Captioning AI Project - Capstone Organization Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0VMdBGC8QuQ"
      },
      "source": [
        "# Assistive Vision Image Captioning AI - Project Organization Notebook\n",
        "\n",
        "by Ashanti Jabri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doiNtIVLnrP2"
      },
      "source": [
        "# 1) High Level Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtsxLJ5zqrtN"
      },
      "source": [
        "- **Be robust enough to provide a guideline that is repeatable**. I don’t want to have to reinvent the wheel every time I want to tackle a similar project.\n",
        "\n",
        "- **Be dynamic enough to be flexible**.  The process acts as a guideline – I'll sometimes have to add/remove/change pieces of it in order to complete the work, whcih can be time consuming if I am not deligent and organized.\n",
        "\n",
        "- **Be iterative** – should be able to start from scratch if needed, without \"really\" starting from scratch. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yruW5yGXn7Ah"
      },
      "source": [
        "# 2) Problem Framing \n",
        "\n",
        "- **Problem**: \\\n",
        "\n",
        " - When we think of the two words, \"assisted\" and \"vision\", a few things come to mind; The visually impaired, high-tech glasses with infrared capabilities and sci-fi, Terminator Esque visual assessment. Though the last two were in jest, assisted vision research inches us ever closer to making those types of ideas realities. In a nutshell, assisted vision is a form of artificial intelligence that allows us to interact with our visual world in more interactive, intuitive, and intelligent ways, letting the machine \"see\" for us. And for obvious reasons, the research and advancements could prove to be especially beneficial to the visually impaired and blind community.\\\n",
        "285 million people globally are visually impaired, 39 million of whom are blind. An analysis of the 1999 Survey of Income and Program Participation (CDC, 2001) revealed blindness or vision problems to be among the top 10 disabilities among adults aged 18 years and older. Vision loss has serious consequences for the individual as well as those who care for and about people who have compromised vision, because it impedes the ability to read, drive, prepare meals, watch television, and attend to personal affairs. Reduced vision among mature adults has been shown to result in social isolation, family stress, and ultimately a greater tendency to experience other health conditions or die prematurely.\\\n",
        "Assisted vision can aid in adding accessibility for the entire community through interactive technologies, however, those same technologies can potentially prove to be beneficial for various disenfranchised groups as well. Globally, 12% of the world are considered functionally illiterate and of that 126 million are children (source: UIS, 2014). A large factor lending to mass illiteracy is the lack of qualified teachers in rural areas. With assisted vision technology, a highly trained model can be just as good as a human in teaching children the basics of reading and speech through automated captioning coupled with text-to-speech technology. \n",
        "\n",
        "\n",
        "- **Porposed Solution**:  \n",
        " - The field of assisted vision technology is expansive, though still relatively untapped in terms of its potential. Many papers and journals have been published expounding on the subject, though few tackle the added functionality of incorporating machine conversational capabilities and text to speech integration. I intend to add to the conversation and lay the groundwork for future research and development by building and deploying an auto-captioning, assisted vision model, then subsequently incorporating said model into a conversational AI that can respond to simple questions about the image using speech to text. \n",
        "\n",
        "- **Ideal Outcome, and Output Type (Classification, Regression or Clustering)**: \n",
        " - The ideal outcome would be deploying a web hosted model that can generate human like, accurate image descriptions and conversational responses through speech-to-text and text-to-speech. \n",
        "\n",
        "- **Success Metrics**:\n",
        " - Success will be based on the interpreability of the AI's initial descriptions and responses. Because the target variable isn't quantifiable, it largely depends on the models ability to \"sound\" human, or at the very minimum to give responses that can be understood by humans.\n",
        "\n",
        "- **Evaluation Metrics**:\n",
        "\n",
        " - Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. For that reason there are a wide range of project specific evaluation metrics that will be used to evaluate our model:\n",
        "\n",
        "   - *Ramakrishna Vedantam, et al. of Cornell University proposed an automated metric, Consensus-based Image Description Evaluation (CIDEr) that evaluates the quality of the description based on captured human concensus. A publically available version of the metric, the CIDEr-D, is available through Microsoft's COCO evaluation Server. *\n",
        "   \n",
        "   - *METEOR, or the Metric for Evaluation of Translation with Explicit ORdering, is a metric for the evaluation of machine translation output. The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching.*\n",
        "\n",
        "   - *BLEU, or the bilingual evaluation understudy,  is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human.*\n",
        "\n",
        "   - *ROUGE, or the Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.*\n",
        "\n",
        "   - *NIST is a method for evaluating the quality of text which has been translated using machine translation. It is based on the BLEU metric, but with some alterations. Where BLEU simply calculates n-gram precision adding equal weight to each one, NIST also calculates how informative a particular n-gram is. That is to say when a correct n-gram is found, the rarer that n-gram is, the more weight it will be given.*\n",
        "\n",
        "- **Final Problem Statement**: \n",
        "\n",
        " - Blindness and illiteracy are two seemingly disconnected global issues, both responsible for the degradation of the quality of life for millions around the world. However, unrelated they may be, I propose that with the aid of technology, both groups can enjoy better lives, increased quality of living, and new, better opportunities to live as normal lives as possible. Though the road towards tackling these issues is a long one, I hypothesize that with Deep Learning we can begin making strides on that path. I set out to use a myriad of machine learning techniques to build an application that can generate image descriptions and perform simple conversational tasks through human speech about those images. This project will incorporate the use of a Convolutional Neural Network (CNN), Recurrent Neural Network encoder-decoder with Bahdanau attention (RNN), Natural language Processing (NLP), Google-Text-To-Speech (gTTS), and ChatterBot. Finally, I plan to deploy a development stage web-hosted version of the model using either Flask or Streamlit (depending on size restraints).\n",
        "\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAfTUIGojKb"
      },
      "source": [
        "# 3) Data Construction \n",
        "\n",
        "* Link to raw dataset(s) here:\n",
        "  - [Training Images](http://images.cocodataset.org/zips/train2014.zip)\n",
        "\n",
        "  - [Testing Images](http://images.cocodataset.org/zips/test2014.zip)\n",
        "\n",
        "  - [Validation Images](http://images.cocodataset.org/zips/val2014.zip)\n",
        "\n",
        "  - [Image Caption Annotation](http://images.cocodataset.org/zips/val2014.zip)\n",
        "\n",
        "  - [Yandex.Toloka NLP Dataset](http://convai.io/data/data_tolokers.json)\n",
        "\n",
        "  - [DeepHack.Chat NLP Dataset](summer_wild_evaluation_dialogs.json.)\n",
        "\n",
        "* 110k Images total, 313k image captions, 5000 conversations.\n",
        "\n",
        "- Data Citations/Sources:\n",
        " - [Common Objects in Context Website](https://cocodataset.org/#download/)\n",
        " - [ConvAI3 Website](http://convai.io/data/)\n",
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBMcPqERsy0F"
      },
      "source": [
        "# 4) Final Dataset \n",
        "\n",
        "- Data Dictionary: \n",
        "\n",
        "- Special Comments concerning Feature Selection/Transformation: None\n",
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgS0u5fSvda-"
      },
      "source": [
        "# 5) Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZLpKUTKvsv9"
      },
      "source": [
        "- Modeling Process: \n",
        " - To begin, we have to create a sequence to sequence model that can handle the task of both learning images and text. The Bahdanau Attention for RNN encoder-decoder model is what we'll use for this initial step, to train a model to generate captions for images.\n",
        "\n",
        "  - Next, we must create a more robust conversational vocabulary with an entirely new model, using an RNN with either the GLovE or Word2vec word embeddings.\n",
        "\n",
        " - Finally, the gTTS API needs to be intergrated before deployment\n",
        "\n",
        "\n",
        "- Final Model: \n",
        "\n",
        "- Metrics to Optomize for and why: \n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhT2kXyDv1Xd"
      },
      "source": [
        "# 6) Model Evaluation \n",
        "\n",
        "- Scores & Predictions: \n",
        "\n",
        "- Interpretation: \n",
        "\n",
        "- Causal Inference (if applicable):\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRa-gF1lyO8y"
      },
      "source": [
        "# 7) Deployment \n",
        "\n",
        "- Types of Deployment:\n",
        "\n",
        "  - *Deploy on Existing Architecture.*\n",
        "\n",
        "  - *Deploy in a new architecture created specifically for the project.*\n",
        "  - *Provide code package in handover to business as Jupyter Notebooks.*\n",
        "\n",
        "  - *Host as an API in a third party app.*\n",
        "\n",
        "  - *Integrate into Business Information delivery tool (BI)* [*More info on this Here*](https://mopinion.com/business-intelligence-bi-tools-overview/)\n",
        "\n",
        "- [Data Iku Deployment Service](https://www.dataiku.com)\n",
        "\n",
        "- [Streamlit I/O Deployment Service](https://streamlit.io)\n",
        "\n",
        "- [ML Flow Complete Machine Learning Tool](https://mlflow.org)\n",
        "\n",
        "- [Flask](https://flask.palletsprojects.com/en/1.1.x/)\n",
        "\n",
        "- [Django](https://www.djangoproject.com)\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nUXV2DW3-j5"
      },
      "source": [
        "# 8) Useful Resources\n",
        "\n",
        "- [Source Code For Various Graphs](https://www.python-graph-gallery.com)\n",
        "\n",
        "- [Data Science Cheatsheet](https://drive.google.com/file/d/1Ho-EqWb1MtOZx0dCsJ3LvEDp-2RzUpBz/view?usp=sharing)\n",
        "\n",
        "- [Machine Learning Cheetsheet](https://drive.google.com/file/d/1-52imsJuvku78b-J4JQGcM7oblzDxrs_/view?usp=sharing)\n",
        "\n",
        "- [Google's Machine Learning Educational Materials](https://developers.google.com/machine-learning/crash-course)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2lBOqfIf5P0"
      },
      "source": [
        "# 9) Sources\n",
        " - [Bahdanau Attention Documentation](https://d2l.ai/chapter_attention-mechanisms/bahdanau-attention.html)\n",
        "\n",
        " - [Haoran Wang, Yue Zhang, and Xiaosheng Yu: An Overview of Image Caption Generation Methods](https://www.hindawi.com/journals/cin/2020/3062706/)\n",
        "\n",
        " - [Evergreen Team Blog](https://evergreen.team/articles/automatic-image-captioning.html)\n",
        "\n",
        "- [IBM Research Blog](https://www.ibm.com/blogs/research/2020/07/image-captioning-assistive-technology/)\n",
        "\n",
        "- [Visual Dialog](https://visualdialog.org)\n",
        "\n",
        "- [Department of Computer Science, Cornell University: Learning to Evaluate Image Captioning](https://vision.cornell.edu/se3/wp-content/uploads/2018/03/1501.pdf)\n",
        "\n",
        "- [Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio: Neural Machine Translation by Jointly Learning to Align and Translate ](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "\n",
        "- [Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh: CIDEr: Consensus-based Image Description Evaluation](https://arxiv.org/abs/1411.5726)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}